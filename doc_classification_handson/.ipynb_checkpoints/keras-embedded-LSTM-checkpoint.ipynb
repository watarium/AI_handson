{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import utils, preprocessing\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# This code was tested with TensorFlow v1.4\n",
    "print(\"You have TensorFlow version\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The CSV was generated from this query: https://bigquery.cloud.google.com/savedquery/513927984416:c494494324be4a80b1fc55f613abb39c\n",
    "# The data is also publicly available at this Cloud Storage URL: https://storage.googleapis.com/tensorflow-workshop-examples/stack-overflow-data.csv\n",
    "data = pd.read_csv(\"owasp10.csv\")\n",
    "token_data = 'owasp10_token.pickle'\n",
    "encode_data = 'owasp10_encode.pickle'\n",
    "model_data = 'owasp10_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_num = data['tags'].nunique()\n",
    "data['tags'].value_counts()\n",
    "\n",
    "max_words  = 10000\n",
    "tokenizer = text.Tokenizer(num_words=max_words, char_level=False)\n",
    "\n",
    "max_len = 50\n",
    "tokenizer.fit_on_texts(data['post'])\n",
    "sequences = tokenizer.texts_to_sequences(data['post'])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data_sec = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "import pickle\n",
    "# save the token data if you want to\n",
    "with open(token_data, 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Split data into train and test\n",
    "train_size = int(len(data_sec) * .8)\n",
    "print (\"Train size: %d\" % train_size)\n",
    "print (\"Test size: %d\" % (len(data_sec) - train_size))\n",
    "\n",
    "x_train = data_sec[:train_size]\n",
    "x_test = data_sec[train_size:]\n",
    "\n",
    "test_posts_doc = data['post'][train_size:]\n",
    "\n",
    "train_tags = data['tags'][:train_size]\n",
    "test_tags = data['tags'][train_size:]\n",
    "\n",
    "# Use sklearn utility to convert label strings to numbered index\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "\n",
    "# save the encoder if you want to\n",
    "with open(encode_data, 'wb') as handle:\n",
    "    pickle.dump(encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 128, input_length=max_len))\n",
    "#model.add(Flatten())\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(tag_num, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.fit trains the model# model. \n",
    "# The validation_split param tells Keras what % of our training data should be used in the validation set\n",
    "# You can see the validation loss decreasing slowly when you run this\n",
    "# Because val_loss is no longer decreasing we stop training to prevent overfitting\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=2,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate the accuracy of our trained model\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=32, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "#save model if you want to\n",
    "model.save(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here's how to generate a prediction on individual examples\n",
    "text_labels = encoder.classes_ \n",
    "for i in range(3):\n",
    "    with open(token_data, 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction)]\n",
    "    print(test_posts_doc.iloc[i][:50], \"...\")\n",
    "    print('Actual label:' + test_tags.iloc[i])\n",
    "    print('Predicted label: ' + predicted_label + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
